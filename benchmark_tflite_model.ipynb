{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized model benchmarking\n",
    "Computation of the accuracies of the quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/animals/test/'\n",
    "TEST_DATA_DIR = './test-hippo/'\n",
    "MODEL_PATH = \"./models/mobilenet-hippo.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite-hippo/\"\n",
    "# choose the model type here\n",
    "QUANT_TYPE = \"both_int8\" # no_quant, w_int8, w_fp16, both_int8, both_fp16\n",
    "TEST_SAMPLES = 300\n",
    "NUM_CLASSES = 3\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "LABELS = [\"cats\", \"dogs\", \"panda\"]\n",
    "LABELS = [\"hippo\", \"other\"]\n",
    "QUANT_NAME_MAP = {\"no_quant\": \"no quantization\", \"w_int8\": \"weights 8-bit INT quantized\", \n",
    "                  \"w_fp16\": \"weights 16-bit FP quantized\", \"both_int8\": \"both weights and activations INT8 quantized\", \n",
    "                 \"both_fp16\": \"both weights and activations FP-16 quantized\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load quantized model according to the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUANT_TYPE == \"no_quant\":\n",
    "    # model without any quantization\n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_no_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"w_int8\":\n",
    "    # model with weights INT8 quantization\n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_weights_int8_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"w_fp16\":\n",
    "    # model with weights FP16 quantization \n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_weights_float16_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"both_int8\":\n",
    "    # model with both weights and activations INT8 quantization \n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_both_int8_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"both_fp16\":\n",
    "    # model with both weights and activations INT8 quantization \n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_both_fp16_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "\n",
    "else:\n",
    "    print(f\"Wrong quantization type has been chosen for {QUANT_NAME_MAP[QUANT_TYPE]}\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of input and output tensors for each model \n",
    "input_ind = interpret.get_input_details()[0][\"index\"]\n",
    "out_ind   = interpret.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for using interpreter: \n",
    "- (check evaluate the models): https://www.tensorflow.org/lite/performance/post_training_float16_quant \n",
    "- it uses batch generator and batch predictions (TODO later): https://thinkmobile.dev/testing-tensorflow-lite-image-classification-model/ \n",
    "- TinyML book chapter on Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TFLite inference on a desktop is slower simply due to it is optimized for mobile devices (ARM processors) and not for Intel processors. For more details, see: \n",
    "- https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save the predicted class label (highest probability one) in a list\n",
    "print(f\"computing results for {QUANT_NAME_MAP[QUANT_TYPE]} ... \\n\")\n",
    "pred = []\n",
    "true = []\n",
    "accuracy_count = 0\n",
    "for i in range(TEST_SAMPLES): \n",
    "    \n",
    "    print(f\"computing results for {i}th image ...\")\n",
    "    \n",
    "    # generate a batch of images \n",
    "    test_image = test_generator.next() \n",
    "    \n",
    "    # set the input image to the input index \n",
    "    interpret.set_tensor(input_ind, test_image[0]) \n",
    "    \n",
    "    # run the inference \n",
    "    interpret.invoke() \n",
    "    \n",
    "    # read the predictions from the output tensor\n",
    "    predictions = interpret.tensor(out_ind) # or, get_tensor(out_ind)\n",
    "    \n",
    "    # get the highest predicted class\n",
    "    pred_class = np.argmax(predictions()[0])\n",
    "    true_class = test_generator.classes[i]\n",
    "    \n",
    "    #print(\"predicted class: \", pred_class, \" and actual class: \", test_generator.classes[i])\n",
    "    \n",
    "    pred.append(pred_class)\n",
    "    true.append(true_class)\n",
    "    \n",
    "    if pred_class == test_generator.classes[i]:\n",
    "        accuracy_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy percentage\n",
    "print(f\"accuracy percentage for {QUANT_NAME_MAP[QUANT_TYPE]}: {round((accuracy_count / TEST_SAMPLES) * 100, 3)}% \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report\n",
    "print(\"-\"*50)\n",
    "print(f\"Confusion matrix for {QUANT_NAME_MAP[QUANT_TYPE]}: \\n\")\n",
    "print(confusion_matrix(y_true=true, y_pred=pred))\n",
    "print(\"-\"*50)\n",
    "print(f\"Classification report for {QUANT_NAME_MAP[QUANT_TYPE]}: \\n\")\n",
    "print(classification_report(y_true=true, y_pred=pred, target_names=LABELS))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
