{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "Running ML model and making inference on mobile devices or embedded devices comes with certain challenges such as the limited amount of resources such as memory, power and data storage. Therefore it is crucial and critical to deploy only optimized and compressed ML models on devices. \n",
    "\n",
    "Tensorflow Lite (TFLite) provides following strategies for optimizing/quantizing the model: \n",
    "\n",
    "- Post training quantization - quantization after model is trained. \n",
    "- Quantization-aware model training - quantization strategy during model's training. \n",
    "\n",
    "Followings are the various options in Post-training quantization: \n",
    "\n",
    "* No Quantization - convert TF model to TF Lite (.tflite) without any modifications in weights and activations values. This will not have any effect on the model, however, it enables you to load and use the model (.tflite) in your mobile devices. Note: size of this file will going to be a bit less than the original h5 file because of using FlatBuffers. \n",
    "\n",
    "\n",
    "* Quantized model (only weights) - it is also called a hybrid approach and here we only quantized weights of the trained model. Now, here you can quantize your model either to 16 bit floating point or 8 bit integer from 32 bit floating point. This will compress the model either 2x or 4x times, respectively. However, during the CPU inference, these 16 bit or 8 bit values will be type-casted again to 32 bits for the computation purposes.\n",
    "\n",
    "\n",
    "* Full integer quantizations of weights and activations - in addition to weight's quantization, here activation values are also quantized to either 16FP or 8INT. Now for example for 8INT, values will be represented from -128 to 127. Therefore, scaling parameters are used to map 32FP to 8INT values. These scaling parameters can be estimated from your representative dataset (ex - validation or test dataset).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TEST_DATA_DIR = './test/'\n",
    "MODEL_PATH = \"./models/mobilenet.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite/\"\n",
    "TEST_SAMPLES = 450\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to save the tflite models if it does not exists\n",
    "if not os.path.exists(TFLITE_MODEL_DIR):\n",
    "    os.makedirs(TFLITE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Conversion to TFLITE without quantization\n",
    "\n",
    "Reference - Check check to TFLIte model:https://www.tensorflow.org/lite/performance/post_training_integer_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_no_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversion to TFLITE with quantization (FP16 and INT8) of weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Conversion to INT8 \n",
    "Reference - Check weight quantization section: https://www.tensorflow.org/lite/performance/post_training_quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "# Note: INT8 quantization is by default! \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_int8_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 4x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Conversion to Float16\n",
    "Reference - https://www.tensorflow.org/lite/performance/post_training_float16_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# set optimization to DEFAULT and set float16 as the supported type on \n",
    "# the target platform\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_float16_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 2x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversion to TFLITE with quantization (FP16 and INT8) of both weights and activations\n",
    "\n",
    "Because a single byte can only represent numbers from -128 to +127, the conversion of activations from floating point (32) to integers / fp-16 requires a calibration step to determine scaling parameters. This is done by running several examples of the input data (ex - testing data) through the floating point model. This gives an estimate of the Max and Min values of the activations. These MIN and MAX values are used to map any real-valued number to -128 and +127.\n",
    "\n",
    "References: \n",
    "- https://www.tensorflow.org/lite/performance/post_training_integer_quant \n",
    "- Full integer quantization of weights and activations: https://www.tensorflow.org/model_optimization/guide/quantization \n",
    "- How to enable post training quantization: https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image generator with a batch size of 1 \n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(TEST_DATA_DIR, \n",
    "                                                  target_size=(IMG_WIDTH, IMG_HEIGHT), \n",
    "                                                  batch_size=1, shuffle=False, \n",
    "                                                  class_mode='categorical')\n",
    "def represent_data_gen():\n",
    "    \"\"\" it yields an image one by one \"\"\"\n",
    "    for ind in range(len(test_generator.filenames)):\n",
    "        img_with_label = test_generator.next() # it returns (image and label) tuple\n",
    "        yield [np.array(img_with_label[0], dtype=np.float32, ndmin=2)] # return only image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conversion to INT8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# assign the custom image generator fn to representative_dataset\n",
    "converter.representative_dataset = represent_data_gen \n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_both_quant_file = TFLITE_MODEL_DIR + \"mobilenet_both_int8_quant.tflite\"\n",
    "with open(tflite_both_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Conversion to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16] # save them in float16\n",
    "converter.representative_dataset = represent_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_both_quant_file = TFLITE_MODEL_DIR + \"mobilenet_both_fp16_quant.tflite\"\n",
    "with open(tflite_both_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
