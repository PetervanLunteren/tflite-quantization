{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "Running ML model and making inference on mobile devices or embedded devices comes with certain challenges such as the limited amount of resources such as memory, power and data storage. Therefore it is crucial and critical to deploy only optimized and compressed ML models on devices. \n",
    "\n",
    "Tensorflow Lite (TFLite) provides following strategies for optimizing/quantizing the model: \n",
    "\n",
    "- Post training quantization - quantization after model is trained. \n",
    "- Quantization-aware model training - quantization strategy during model's training. \n",
    "\n",
    "Followings are the various options in Post-training quantization: \n",
    "\n",
    "* No Quantization - convert TF model to TF Lite (.tflite) without any modifications in weights and activations values. This will not have any effect on the model, however, it enables you to load use the model now (.tflite) in your mobile devives. Note: size of this file will going to be a bit less than the original h5 file because of using FlatBuffers. \n",
    "\n",
    "\n",
    "* Quantized model (but only weights) - it is also called a hybrid approach and here we only quantized weights of the trained model. Now, here you can quantize your model either to 16 bit floating point or 8 bit integer from 32 bit floating point. This will compress the model either 2x or 4x times, respectively. However, during the inference time, these 16 bit or 8 bit values will be type-casted again to 32 bits for the computation purposes. Because activation values will be still computed in 32 bit FP. \n",
    "\n",
    "\n",
    "* Full integer quantizations of weights and activations - in addition to weight's quantization, here activation values are also quantized to either 16FP or 8INT. Now for example for 8INT, values will be represented from -128 to 127. Therefore, a scaling is calculated which maps 32FP to 8INT values. These scaling parameters are then found from your representative dataset (ex - validation or test dataset).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from vis.utils import utils\n",
    "from vis.visualization import visualize_cam, overlay\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/animals/test/'\n",
    "MODEL_PATH = \"./models/mobilenet.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite/\"\n",
    "TEST_SAMPLES = 450\n",
    "NUM_CLASSES = 3\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "LABELS = [\"cats\", \"dogs\", \"panda\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to save the tflite models if it does not exists\n",
    "if not os.path.exists(TFLITE_MODEL_DIR):\n",
    "    os.makedirs(TFLITE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Conversion to TFLITE without quantization\n",
    "\n",
    "Reference - Check check to TFLIte model:https://www.tensorflow.org/lite/performance/post_training_integer_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_no_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: there should be hardly reduction in size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversion to TFLITE with quantization (FP16 and INT8) of weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Conversion to INT8 \n",
    "Reference - Check weight quantization section: https://www.tensorflow.org/lite/performance/post_training_quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "# Note INT8 quantization is by default! \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_int8_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 4x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Conversion to Float16\n",
    "Reference - https://www.tensorflow.org/lite/performance/post_training_float16_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# set optimization to DEFAULT and set float16 as the supported type on the target platform\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_float16_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 2x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversion to TFLITE with quantization (FP16 and INT8) of both weights and activations\n",
    "\n",
    "Because a single byte can only represent numbers from -128 to +127, the conversion of activations from floating point (32) to integers / fp-16 requires a calibration step to determine scaling parameters. This is done by running several examples of the input data (ex - testing data) through the floating point model. This gives an estimate of the Max and Min values of the activations. These extremes are mapped to -128 and +127 respectively to calculate the scaling parameters. \n",
    "\n",
    "References: \n",
    "- https://www.tensorflow.org/lite/performance/post_training_integer_quant \n",
    "- Full integer quantization of weights and activations: https://www.tensorflow.org/model_optimization/guide/quantization \n",
    "- How to enable post training quantization: https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a test image generator with a batch size of 1 \n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    \"\"\" it yields / generates a testing image one by one \"\"\"\n",
    "    for ind in range(len(test_generator.filenames)):\n",
    "        img = test_generator.next() # it returns both image and label in a tuple\n",
    "        #print(f\"image yielded {ind} with dim: {img[0].shape}\") # for debug only\n",
    "        yield [np.array(img[0], dtype=np.float32, ndmin=2)]\n",
    "        \n",
    "# For debugging only   \n",
    "#num_imgs = 2\n",
    "#for val in range(num_imgs):\n",
    "#    img = next(representative_data_gen())\n",
    "#    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conversion to INT8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# assign the representative data generator to representative_dataset\n",
    "converter.representative_dataset = representative_data_gen \n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_both_quant_file = TFLITE_MODEL_DIR + \"mobilenet_both_int8_quant.tflite\"\n",
    "with open(tflite_both_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Conversion to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16] # save them in float16\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_both_quant_file = TFLITE_MODEL_DIR + \"mobilenet_both_fp16_quant.tflite\"\n",
    "with open(tflite_both_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GPU Delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
